# Sort by frequency
ngram1.df <- sort(rowSums(as.data.frame(as.matrix(docfreq(ngram1.dfm)))), decreasing=TRUE)
ngram2.df <- sort(rowSums(as.data.frame(as.matrix(docfreq(ngram2.dfm)))), decreasing=TRUE)
ngram3.df <- sort(rowSums(as.data.frame(as.matrix(docfreq(ngram3.dfm)))), decreasing=TRUE)
# Coerce into df - ngram sorted by Word and Frequency
ng1.df  <- data.frame(Words=names(ngram1.df), Frequency = ngram1.df, N=1, stringsAsFactors = F)
ng2.df <- data.frame(Words=names(ngram2.df), Frequency = ngram2.df, N=2, stringsAsFactors = F)
ng3.df  <- data.frame(Words=names(ngram3.df), Frequency = ngram3.df, N=3, stringsAsFactors = F)
# add add'l column for except-last-word (eg "A glass of" = "A glass" )
ng1.df$exceptlastword <- ng1.df$Words
ng2.df$exceptlastword <-  sapply(strsplit(ng2.df$Words, '_'), function(a) a[1])
ng3.df$exceptlastword <-  sapply(strsplit(ng3.df$Words, '_'), function(a) paste(a[1], a[2], sep = '_'))
# Calculate frequency of frequencies for all n-grams
Freq1.df <- data.frame(Words=names(ng1.df), Frequency = ng1.df, ngram = 1, stringsAsFactors = F)
ng1.freqfreq = count(Freq1.df, 'Frequency')
names(ng1.freqfreq) = c('r', 'n')
Freq2.df <- data.frame(Words=names(ng2.df), Frequency = ng2.df, ngram = 2, stringsAsFactors = F)
ng2.freqfreq = count(Freq2.df, 'Frequency')
names(ng2.freqfreq) = c('r', 'n')
Freq3.df <- data.frame(Words=names(ng3.df), Frequency = ng3.df, ngram = 3, stringsAsFactors = F)
ng3.freqfreq = count(Freq3.df, 'Frequency')
names(ng3.freqfreq) = c('r', 'n')
# housecleaning
rm(ngram1, ngram2, ngram3, ngram1.df, ngram2.df, ngram3.df, ngram1.dfm, ngram2.dfm, ngram3.dfm, text.sample)
rm(ngram1, ngram2, ngram3,
ngram1.df, ngram2.df, ngram3.df,
ngram1.dfm, ngram2.dfm, ngram3.dfm,
text.sample, raw_text, split_text, word_count)
rm(Freq1.df, Freq2.df, Freq3.df)
# save workspace for use with Data Product
save.image("~/R/DataScience-JohnsHopkins-Capstone/Task_3_Modeling/Task3_Complete.RData")
save.image("~/R/DataScience-JohnsHopkins-Capstone/Task_6_ DataProduct/data/DataApp.RData")
Freq1.df <- data.frame(Words=names(ng1.df), Frequency = ng1.df, ngram = 1, stringsAsFactors = F)
# Calculate frequency of frequencies for all n-grams
Freq1.df <- data.frame(Words=names(ngram1.df), Frequency = ngram1.df, ngram = 1, stringsAsFactors = F)
ng1.freqfreq = count(Freq1.df, 'Frequency')
names(ng1.freqfreq) = c('r', 'n')
?count
library(plyr, warn.conflicts = FALSE, quietly=TRUE)
############################################################
# Capstone Project -Task 3 Modeling
# Data Scientist - Eric Bruce
# Date Mar 2016
# R 1.0
#
# Algorithims for predicting the next phrase
#
#############################################################
############################################################
# Libraries
############################################################
library(stringr); library(stringi); library(R.utils)
library(qdap, warn.conflicts = FALSE, quietly=TRUE)
library(tm, warn.conflicts = FALSE, quietly=TRUE)
library(quanteda, warn.conflicts = FALSE, quietly=TRUE)
library(ggplot2, warn.conflicts = FALSE, quietly=TRUE)
library(plyr, warn.conflicts = FALSE, quietly=TRUE)
library(stats)
############################################################
# Open Corpus, create ngram & ngram frequency of frequencies
############################################################
load("~/R/DataScience-JohnsHopkins-Capstone/Task_3_Modeling/Task1_Complete.RData")
# Create ngram
ngram1 <- tokenize(toLower(text.sample),
removePunct = TRUE,
removeNumbers = TRUE,
removeTwitter = TRUE,
ngrams = 1)
ngram2 <- tokenize(toLower(text.sample),
removePunct = TRUE,
removeNumbers = TRUE,
removeTwitter = TRUE,
ngrams = 2)
ngram3 <- tokenize(toLower(text.sample),
removePunct = TRUE,
removeNumbers = TRUE,
removeTwitter = TRUE,
ngrams = 3)
# Create dfm
ngram1.dfm <- dfm(ngram1)
ngram2.dfm <- dfm(ngram2)
ngram3.dfm <- dfm(ngram3)
# Sort by frequency
ngram1.df <- sort(rowSums(as.data.frame(as.matrix(docfreq(ngram1.dfm)))), decreasing=TRUE)
ngram2.df <- sort(rowSums(as.data.frame(as.matrix(docfreq(ngram2.dfm)))), decreasing=TRUE)
ngram3.df <- sort(rowSums(as.data.frame(as.matrix(docfreq(ngram3.dfm)))), decreasing=TRUE)
# Calculate frequency of frequencies for all n-grams
Freq1.df <- data.frame(Words=names(ngram1.df), Frequency = ngram1.df, ngram = 1, stringsAsFactors = F)
ng1.freqfreq = count(Freq1.df, 'Frequency')
names(ng1.freqfreq) = c('r', 'n')
View(ng1.freqfreq)
############################################################
# Capstone Project -Task 3 Modeling
# Data Scientist - Eric Bruce
# Date Mar 2016
# R 1.0
#
# Algorithims for predicting the next phrase
#
#############################################################
############################################################
# Libraries
############################################################
library(stringr); library(stringi); library(R.utils)
library(qdap, warn.conflicts = FALSE, quietly=TRUE)
library(tm, warn.conflicts = FALSE, quietly=TRUE)
library(quanteda, warn.conflicts = FALSE, quietly=TRUE)
library(ggplot2, warn.conflicts = FALSE, quietly=TRUE)
library(plyr, warn.conflicts = FALSE, quietly=TRUE)
library(stats)
############################################################
# Open Corpus, create ngram & ngram frequency of frequencies
############################################################
load("~/R/DataScience-JohnsHopkins-Capstone/Task_3_Modeling/Task1_Complete.RData")
# Create ngram
ngram1 <- tokenize(toLower(text.sample),
removePunct = TRUE,
removeNumbers = TRUE,
removeTwitter = TRUE,
ngrams = 1)
ngram2 <- tokenize(toLower(text.sample),
removePunct = TRUE,
removeNumbers = TRUE,
removeTwitter = TRUE,
ngrams = 2)
ngram3 <- tokenize(toLower(text.sample),
removePunct = TRUE,
removeNumbers = TRUE,
removeTwitter = TRUE,
ngrams = 3)
# Create dfm
ngram1.dfm <- dfm(ngram1)
ngram2.dfm <- dfm(ngram2)
ngram3.dfm <- dfm(ngram3)
# Sort by frequency
ngram1.df <- sort(rowSums(as.data.frame(as.matrix(docfreq(ngram1.dfm)))), decreasing=TRUE)
ngram2.df <- sort(rowSums(as.data.frame(as.matrix(docfreq(ngram2.dfm)))), decreasing=TRUE)
ngram3.df <- sort(rowSums(as.data.frame(as.matrix(docfreq(ngram3.dfm)))), decreasing=TRUE)
# Calculate frequency of frequencies for all n-grams
Freq1.df <- data.frame(Words=names(ngram1.df), Frequency = ngram1.df, ngram = 1, stringsAsFactors = F)
ng1.freqfreq = count(Freq1.df, 'Frequency')
names(ng1.freqfreq) = c('r', 'n')
Freq2.df <- data.frame(Words=names(ngram2.df), Frequency = ngram2.df, ngram = 2, stringsAsFactors = F)
ng2.freqfreq = count(Freq2.df, 'Frequency')
names(ng2.freqfreq) = c('r', 'n')
Freq3.df <- data.frame(Words=names(ngram3.df), Frequency = ngram3.df, ngram = 3, stringsAsFactors = F)
ng3.freqfreq = count(Freq3.df, 'Frequency')
names(ng3.freqfreq) = c('r', 'n')
# Coerce into df - ngram sorted by Word and Frequency
ng1.df  <- data.frame(Words=names(ngram1.df), Frequency = ngram1.df, N=1, stringsAsFactors = F)
ng2.df <- data.frame(Words=names(ngram2.df), Frequency = ngram2.df, N=2, stringsAsFactors = F)
ng3.df  <- data.frame(Words=names(ngram3.df), Frequency = ngram3.df, N=3, stringsAsFactors = F)
# add add'l column for except-last-word (eg "A glass of" = "A glass" )
ng1.df$exceptlastword <- ng1.df$Words
ng2.df$exceptlastword <-  sapply(strsplit(ng2.df$Words, '_'), function(a) a[1])
ng3.df$exceptlastword <-  sapply(strsplit(ng3.df$Words, '_'), function(a) paste(a[1], a[2], sep = '_'))
# housecleaning
rm(ngram1, ngram2, ngram3, ngram1.df, ngram2.df, ngram3.df, ngram1.dfm, ngram2.dfm, ngram3.dfm, text.sample)
rm(ngram1, ngram2, ngram3,
ngram1.df, ngram2.df, ngram3.df,
ngram1.dfm, ngram2.dfm, ngram3.dfm,
text.sample, raw_text, split_text, word_count)
rm(Freq1.df, Freq2.df, Freq3.df)
# save workspace for use with Data Product
save.image("~/R/DataScience-JohnsHopkins-Capstone/Task_3_Modeling/Task3_Complete.RData")
save.image("~/R/DataScience-JohnsHopkins-Capstone/Task_6_ DataProduct/data/DataApp.RData")
View(ng3.freqfreq)
library(stringr); library(stringi); library(R.utils)
library(tm, warn.conflicts = FALSE, quietly=TRUE)
library(quanteda, warn.conflicts = FALSE, quietly=TRUE)
library(ggplot2, warn.conflicts = FALSE, quietly=TRUE)
library(stats)
load('./data/DataApp.RData')
############################################################
# Capstone Project -Task 3 Modeling
# Data Scientist - Eric Bruce
# Date Mar 2016
# R 1.0
#
# Algorithims for predicting the next phrase
#
#############################################################
############################################################
# Libraries
############################################################
library(stringr); library(stringi); library(R.utils)
library(qdap, warn.conflicts = FALSE, quietly=TRUE)
library(tm, warn.conflicts = FALSE, quietly=TRUE)
library(quanteda, warn.conflicts = FALSE, quietly=TRUE)
library(ggplot2, warn.conflicts = FALSE, quietly=TRUE)
library(plyr, warn.conflicts = FALSE, quietly=TRUE)
library(stats)
############################################################
# Terms to Block - FrontgateMedia.com
############################################################
terms2block <- read.csv("~/R/DataScience-JohnsHopkins-Capstone/data/other/Terms-to-Block.csv"
, header = FALSE
, stringsAsFactors = FALSE
)
terms2block <- paste0(terms2block$V1, collapse="|")
############################################################
# Open Corpus, create ngram & ngram frequency of frequencies
############################################################
load("~/R/DataScience-JohnsHopkins-Capstone/Task_3_Modeling/Task1_Complete.RData")
# Create ngram
ngram1 <- tokenize(toLower(text.sample),
removePunct = TRUE,
removeNumbers = TRUE,
removeTwitter = TRUE,
ngrams = 1)
ngram2 <- tokenize(toLower(text.sample),
removePunct = TRUE,
removeNumbers = TRUE,
removeTwitter = TRUE,
ngrams = 2)
ngram3 <- tokenize(toLower(text.sample),
removePunct = TRUE,
removeNumbers = TRUE,
removeTwitter = TRUE,
ngrams = 3)
# Create dfm
ngram1.dfm <- dfm(ngram1)
ngram2.dfm <- dfm(ngram2)
ngram3.dfm <- dfm(ngram3)
# Sort by frequency
ngram1.df <- sort(rowSums(as.data.frame(as.matrix(docfreq(ngram1.dfm)))), decreasing=TRUE)
ngram2.df <- sort(rowSums(as.data.frame(as.matrix(docfreq(ngram2.dfm)))), decreasing=TRUE)
ngram3.df <- sort(rowSums(as.data.frame(as.matrix(docfreq(ngram3.dfm)))), decreasing=TRUE)
# Calculate frequency of frequencies for all n-grams
Freq1.df <- data.frame(Words=names(ngram1.df), Frequency = ngram1.df, ngram = 1, stringsAsFactors = F)
ng1.freqfreq = count(Freq1.df, 'Frequency')
names(ng1.freqfreq) = c('r', 'n')
Freq2.df <- data.frame(Words=names(ngram2.df), Frequency = ngram2.df, ngram = 2, stringsAsFactors = F)
ng2.freqfreq = count(Freq2.df, 'Frequency')
names(ng2.freqfreq) = c('r', 'n')
Freq3.df <- data.frame(Words=names(ngram3.df), Frequency = ngram3.df, ngram = 3, stringsAsFactors = F)
ng3.freqfreq = count(Freq3.df, 'Frequency')
names(ng3.freqfreq) = c('r', 'n')
# Coerce into df - ngram sorted by Word and Frequency
ng1.df  <- data.frame(Words=names(ngram1.df), Frequency = ngram1.df, N=1, stringsAsFactors = F)
ng2.df <- data.frame(Words=names(ngram2.df), Frequency = ngram2.df, N=2, stringsAsFactors = F)
ng3.df  <- data.frame(Words=names(ngram3.df), Frequency = ngram3.df, N=3, stringsAsFactors = F)
# add add'l column for except-last-word (eg "A glass of" = "A glass" )
ng1.df$exceptlastword <- ng1.df$Words
ng2.df$exceptlastword <-  sapply(strsplit(ng2.df$Words, '_'), function(a) a[1])
ng3.df$exceptlastword <-  sapply(strsplit(ng3.df$Words, '_'), function(a) paste(a[1], a[2], sep = '_'))
# housecleaning
rm(ngram1, ngram2, ngram3, ngram1.df, ngram2.df, ngram3.df, ngram1.dfm, ngram2.dfm, ngram3.dfm, text.sample)
rm(ngram1, ngram2, ngram3,
ngram1.df, ngram2.df, ngram3.df,
ngram1.dfm, ngram2.dfm, ngram3.dfm,
text.sample, raw_text, split_text, word_count)
rm(Freq1.df, Freq2.df, Freq3.df)
# save workspace for use with Data Product
save.image("~/R/DataScience-JohnsHopkins-Capstone/Task_3_Modeling/Task3_Complete.RData")
save.image("~/R/DataScience-JohnsHopkins-Capstone/Task_6_ DataProduct/data/DataApp.RData")
############################################################
# nGram Models
############################################################
# load workspace
#
library(stringr); library(stringi); library(R.utils)
library(tm, warn.conflicts = FALSE, quietly=TRUE)
library(quanteda, warn.conflicts = FALSE, quietly=TRUE)
library(ggplot2, warn.conflicts = FALSE, quietly=TRUE)
library(stats)
load('./data/DataApp.RData')
shiny::runApp()
View(ng1.freqfreq)
View(ng2.df)
View(ng1.df)
runApp()
runApp()
runApp()
runApp()
input <- "A nice cold glass of" # input_bigram = 'glass_of'
# Process input
input <- tokenize(toLower(input),
removePunct = TRUE,
removeNumbers = TRUE,
removeTwitter = TRUE,
ngrams = 2)
input <- as.data.frame(as.matrix(input), stringsAsFactors = FALSE)
input <- tail(input, 1)[1,1]
if(length(input) == 0){
input <- "Invalid entry"
return(data.frame(next_word=input,score=1.,stringsAsFactors = F))}
input2 <- unlist(strsplit(input,"_"))[2]
D1 <- ng1.freqfreq[1,2]/(ng1.freqfreq[1,2]+2*ng1.freqfreq[2,2])
D2 <- ng2.freqfreq[1,2]/(ng2.freqfreq[1,2]+2*ng2.freqfreq[2,2])
D3 <- ng3.freqfreq[1,2]/(ng3.freqfreq[1,2]+2*ng3.freqfreq[1,2])
seekcw1w2 <- grepl(paste0("^",input,"$"),ng3.df$exceptlastword)
subtri <-ng3.df[seekcw1w2,]
cw1w2 <- sum(subtri$freq)
nw1w2 <- sum(seekcw1w2)
seekW2 <- grepl(paste0("^",input2,"$"),ng2.df$exceptlastword)
W2 <- sum(seekW2)
p3 <- D3*nw1w2/cw1w2
seekW2 <- grepl(paste0(input2,"$"), ng3.df$exceptlastword)
W2 <- sum(seekW2)
p3 <- D3*nw1w2/cw1w2
# get c(w2.), n(w2.) and n(..) from bigram
seekcw2 <- grepl(input2, ng2.df$exceptlastword)
subbi <- ng2.df[seekcw2,]
cw2 <- sum(subbi$freq)
nw2 <- sum(seekcw2)
nw <- nrow(ng2.df)
p2 <- D3*nw2/cw2/nw
p1 <- D2*nw2/cw2/nw
cp <- unique(subbi$name)
pkn <- rep(NA,length(cp))
for(i in 1:length(cp)){
# get nw3 cw3 for smooth
nw3 <- sum(grepl(cp[i], ng2.df$exceptlastword))
cw3 <- subbi[subbi$name == cp[i],2]
pkn[i] <- max((cw3-D2),0)/cw2 + P*nw3
}
View(subbi)
seekcw2 <- grepl(input2, ng2.df$exceptlastword)
subbi <- ng2.df[seekcw2,]
View(subbi)
View(subbi)
seekcw2 <- grepl(input2, ng2.df$exceptlastword)
subbi <- ng2.df[seekcw2,]
cw2 <- sum(subbi$Frequency)
nw2 <- sum(seekcw2)
nw <- nrow(ng2.df)
p2 <- D3*nw2/cw2/nw
p1 <- D2*nw2/cw2/nw
seekcw1w2 <- grepl(paste0("^",input,"$"),ng3.df$exceptlastword)
subtri <-ng3.df[seekcw1w2,]
cw1w2 <- sum(subtri$Frequency)
nw1w2 <- sum(seekcw1w2)
seekW2 <- grepl(paste0("^",input2,"$"),ng2.df$exceptlastword)
W2 <- sum(seekW2)
p3 <- D3*nw1w2/cw1w2
# get c(w1w2.), n(w1w2.) and n(.w2.) from trigram
seekW2 <- grepl(paste0(input2,"$"), ng3.df$exceptlastword)
W2 <- sum(seekW2)
p3 <- D3*nw1w2/cw1w2
seekcw2 <- grepl(input2, ng2.df$exceptlastword)
subbi <- ng2.df[seekcw2,]
cw2 <- sum(subbi$Frequency)
nw2 <- sum(seekcw2)
nw <- nrow(ng2.df)
p2 <- D3*nw2/cw2/nw
p1 <- D2*nw2/cw2/nw
cp <- unique(subbi$exceptlastword)
pkn <- rep(NA,length(cp))
for(i in 1:length(cp)){
# get nw3 cw3 for smooth
nw3 <- sum(grepl(cp[i], ng2.df$exceptlastword))
cw3 <- subbi[subbi$name == cp[i],2]
pkn[i] <- max((cw3-D2),0)/cw2 + P*nw3
}
cp <- unique(subbi$exceptlastword)
pkn <- rep(NA,length(cp))
for(i in 1:length(cp)){
# get nw3 cw3 for smooth
nw3 <- sum(grepl(cp[i], ng2.df$exceptlastword))
cw3 <- subbi[subbi$name == cp[i],2]
pkn[i] <- max((cw3-D2),0)/cw2 + p1*nw3
}
cp <- unique(subbi$exceptlastword)
pkn <- rep(NA,length(cp))
for(i in 1:length(cp)){
# get nw3 nw2w3 and cw1w2w3 for smooth
nw3 <- sum(grepl(cp[i],ng2.df$exceptlastword))
nw2w3 <- sum(grepl(paste0(input2,' ',cp[i],'$'),ng3.df$exceptlastword))
cw1w2w3 <- subtri[subtri$name == cp[i],2]
pkn[i] <- max((cw1w2w3-D3),0)/cw1w2 + p3*(max((nw2w3-D3),0)/W2+ p2*nw3)
}
predictWord <- data.frame(next_word=cp, probability=pkn,stringsAsFactors = F)
predictWord$next_word <- word(predictWord$next_word, -1, sep = fixed('_'))
predictWord <- aggregate(probability ~ next_word, predictWord, max)
View(predictWord)
predictWord[order(predictWord$probability,decreasing = T),][1:maxResults = 5,])
runApp()
mkn <- predict.mkn(input,
terms2block,
ng1.df, ng2.df, ng3.df,
maxResults)
maxResults = 5
mkn <- predict.mkn(input,
terms2block,
ng1.df, ng2.df, ng3.df,
maxResults)
View(mkn)
View(mkn)
predict.mkn <-function(input,terms2block,ng1.df,ng2.df,ng3.df, maxResults){
# Process input
input <- tokenize(toLower(input),
removePunct = TRUE,
removeNumbers = TRUE,
removeTwitter = TRUE,
ngrams = 2)
input <- as.data.frame(as.matrix(input), stringsAsFactors = FALSE)
input <- tail(input, 1)[1,1]
if(length(input) == 0){
input <- "Invalid entry"
return(data.frame(next_word=input,score=1.,stringsAsFactors = F))}
input2 <- unlist(strsplit(input,"_"))[2]
# get D by Ney et al. by the total number of n-grams occurring exactly once (n1) and twice (n2)
D1 <- ng1.freqfreq[1,2]/(ng1.freqfreq[1,2]+2*ng1.freqfreq[2,2])
D2 <- ng2.freqfreq[1,2]/(ng2.freqfreq[1,2]+2*ng2.freqfreq[2,2])
D3 <- ng3.freqfreq[1,2]/(ng3.freqfreq[1,2]+2*ng3.freqfreq[1,2])
# Initialize variables for prediction
seekcw1w2 <- grepl(paste0("^",input,"$"),ng3.df$exceptlastword)
subtri <-ng3.df[seekcw1w2,]
cw1w2 <- sum(subtri$Frequency)
nw1w2 <- sum(seekcw1w2)
seekW2 <- grepl(paste0("^",input2,"$"),ng2.df$exceptlastword)
W2 <- sum(seekW2)
p3 <- D3*nw1w2/cw1w2
# get c(w1w2.), n(w1w2.) and n(.w2.) from trigram
seekW2 <- grepl(paste0(input2,"$"), ng3.df$exceptlastword)
W2 <- sum(seekW2)
p3 <- D3*nw1w2/cw1w2
# get c(w2.), n(w2.) and n(..) from bigram
seekcw2 <- grepl(input2, ng2.df$exceptlastword)
subbi <- ng2.df[seekcw2,]
cw2 <- sum(subbi$Frequency)
nw2 <- sum(seekcw2)
nw <- nrow(ng2.df)
p2 <- D3*nw2/cw2/nw
p1 <- D2*nw2/cw2/nw
if(cw1w2 == 0){
# kick off to unigram if no bigram
if(nw2 == 0) {
return(head(ng1.df[order(ng1.df$Frequency,decreasing = TRUE),1],maxResults))
}
# kick off to 2-gram model
cp <- unique(subbi$Words)
pkn <- rep(NA,length(cp))
for(i in 1:length(cp)){
# get nw3 cw3 for smooth
nw3 <- sum(grepl(cp[i], ng2.df$exceptlastword))
cw3 <- subbi[subbi$Words == cp[i],2]
pkn[i] <- max((cw3-D2),0)/cw2 + p1*nw3
}
}
cp <- unique(subbi$Words)
pkn <- rep(NA,length(cp))
for(i in 1:length(cp)){
# get nw3 nw2w3 and cw1w2w3 for smooth
nw3 <- sum(grepl(cp[i],ng2.df$exceptlastword))
nw2w3 <- sum(grepl(paste0(input2,' ',cp[i],'$'),ng3.df$exceptlastword))
cw1w2w3 <- subtri[subtri$name == cp[i],2]
pkn[i] <- max((cw1w2w3-D3),0)/cw1w2 + p3*(max((nw2w3-D3),0)/W2+ p2*nw3)
}
predictWord <- data.frame(next_word=cp, probability=pkn,stringsAsFactors = F)
predictWord$next_word <- word(predictWord$next_word, -1, sep = fixed('_'))
predictWord <- aggregate(probability ~ next_word, predictWord, max)
return(na.omit(predictWord[order(predictWord$probability,decreasing = T),][1:maxResults,]))
}
############################################################
# Stupid Backoff
############################################################
predict.sbo <-function(input, profanewords, ng1.df, ng2.df, ng3.df, maxResults) {
# Process input
input <- tokenize(toLower(input),
removePunct = TRUE,
removeNumbers = TRUE,
removeTwitter = TRUE,
ngrams = 2)
input <- as.data.frame(as.matrix(input), stringsAsFactors = FALSE)
input <- tail(input, 1)[1,1]
if(length(input) == 0){
input <- "Invalid entry"
return(data.frame(next_word=input,score=1.,stringsAsFactors = F))}
# Initialize variables for prediction
seektri<-grepl(paste0("^",input,"$"),ng3.df$exceptlastword)
subtri<-ng3.df[seektri,]
input2 <- unlist(strsplit(input,"_"))[2]
seekbi <- grepl(paste0("^",input2,"$"),ng2.df$exceptlastword)
subbi <- ng2.df[seekbi,]
ng1.df$s <- ng1.df$Frequency/nrow(ng1.df)*0.16
useuni <- ng1.df[order(ng1.df$s,decreasing = T),]
useunia <- useuni[1:maxResults,]
# Calculate Next Word and Next Word Scores
if (sum(seektri) == 0) {
if(sum(seekbi)==0){
# Use ng1 because ng2 and ng3 cannot provide sol'ns
return(head(na.omit(ng1.df[order(ng1.df$Frequency,decreasing = T),1]),maxResults))
}
# Use ng2 because ng3 cannot provide soln's
subbi$s <- 0.4*subbi$Frequency/sum(seekbi)
names <- c(subbi$Word,useunia$ng1)
score <- c(subbi$s,useunia$s)[1:length(names)]
predictWord <- data.frame(next_word=names,score=score,stringsAsFactors = F)
predictWord$next_word <- word(predictWord$next_word, -1, sep = fixed('_'))
predictWord <- aggregate(score ~ next_word, predictWord, max)
return(na.omit(predictWord[order(predictWord$score,decreasing = T),][1:maxResults,]))
}
# Use ng2 and ng3 - best soln's
subbi$s <- 0.4*subbi$Frequency/sum(seekbi)
subtri$s <- subtri$Frequency/sum(subtri$Frequency)
names <- c(subtri$Words,subbi$Words,useunia$Words)
score <- c(subtri$s,subbi$s,useunia$s)[1:length(names)]
predictWord <- data.frame(next_word=names,score=score,stringsAsFactors = F)
predictWord$next_word <- word(predictWord$next_word, -1, sep = fixed('_'))
predictWord <- aggregate(score ~ next_word, predictWord, max)
return(na.omit(predictWord[order(predictWord$score,decreasing = T),][1:maxResults,]))
}
input <- "A nice cold glass of" # input_bigram = 'glass_of'
maxResults = 5
# profanewords <- c('ass', 'bitch', 'cunt', 'damn', 'fuck')
mkn <- predict.mkn(input,
terms2block,
ng1.df, ng2.df, ng3.df,
maxResults)
View(mkn)
